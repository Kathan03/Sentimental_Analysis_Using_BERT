# Sentimental_Analysis_Using_BERT

The goal of this project is to identify the hidden sentiments in the text written by humans. To this aim, I use the BERT Encoder-Only Large Language Model (LLM) to achive the task. I the pre-trained BERT model, was finetuned for the downstream task of sentimental analysis from text on SMILE Twitter Dataset 

BERT, which stands for Bidirectional Encoder Representations from Transformers, is a pre-trained natural language processing (NLP) model developed by Google researchers in 2018. It represents a significant advancement in NLP by leveraging the Transformer architecture, a deep learning model that has gained prominence for its effectiveness in capturing contextual relationships in sequences.
